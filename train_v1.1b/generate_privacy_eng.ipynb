{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개인정보 패턴 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from faker import Faker\n",
    "from faker.providers import company, phone_number, profile, bank, person, credit_card, passport, ssn\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup Faker to use Korean locale\n",
    "fake = Faker('en')\n",
    "fake.add_provider(company)\n",
    "fake.add_provider(bank)\n",
    "fake.add_provider(phone_number)\n",
    "fake.add_provider(profile)\n",
    "fake.add_provider(person)\n",
    "fake.add_provider(credit_card)\n",
    "fake.add_provider(passport)\n",
    "fake.add_provider(ssn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "selected_model_list = ['upstage/SOLAR-10.7B-Instruct-v1.0', 'meta-llama/Meta-Llama-3-8B-Instruct', 'google/gemma-2-9b-it', 'mistralai/Mistral-7B-Instruct-v0.2']\n",
    "model_name_or_path = selected_model_list[1]\n",
    "# config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "if 'solar' in model_name_or_path.lower():\n",
    "    MODEL = 'solar'\n",
    "elif 'llama' in model_name_or_path.lower():\n",
    "    MODEL = 'llama3'\n",
    "    if '-2' in model_name_or_path.lower():\n",
    "        MODEL = 'llama2'\n",
    "elif 'gemma' in model_name_or_path.lower():\n",
    "    MODEL = 'gemma'\n",
    "elif 'mistral' in model_name_or_path.lower():\n",
    "    MODEL = 'mistral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate a card number\n",
    "def generate_strong_id():\n",
    "    # 첫 문자는 소문자 알파벳이나 숫자\n",
    "    first_characters = string.ascii_lowercase + string.digits\n",
    "    first_char = random.choice(first_characters)\n",
    "    \n",
    "    # 나머지 문자는 소문자 알파벳, 숫자, 언더바 포함\n",
    "    characters = string.ascii_lowercase + string.digits + '_'\n",
    "    remaining_chars = ''.join(random.choices(characters, k=7))\n",
    "    \n",
    "    return first_char + remaining_chars\n",
    "\n",
    "def generate_strong_password():\n",
    "    lower = string.ascii_lowercase\n",
    "    digits = string.digits\n",
    "    special = '!@#$%'\n",
    "    \n",
    "    # 각 그룹에서 최소 하나씩 선택\n",
    "    password = [\n",
    "        random.choice(lower),\n",
    "        random.choice(digits),\n",
    "        random.choice(special)\n",
    "    ]\n",
    "    \n",
    "    # 나머지 자리는 세 그룹의 문자를 모두 포함하여 랜덤하게 선택\n",
    "    all_characters = lower + digits + special\n",
    "    password += random.choices(all_characters, k=9)\n",
    "    \n",
    "    # 비밀번호를 셔플하여 랜덤하게 정렬\n",
    "    random.shuffle(password)\n",
    "    \n",
    "    return ''.join(password)\n",
    "\n",
    "# Helper function to generate ID and password\n",
    "def generate_id_password():\n",
    "    id = generate_strong_id()\n",
    "    password = generate_strong_password()\n",
    "    return id, password\n",
    "\n",
    "def generate_license_number():\n",
    "    patterns = [\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(100, 999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(10000, 999999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(100000, 99999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(1000, 99999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(100000000, 99999999999)}\",\n",
    "        lambda: f\"{''.join(random.choices(string.ascii_uppercase, k=random.randint(1,2)))}{random.randint(10000, 999999)}\",\n",
    "        lambda: f\"H{random.randint(10000000, 99999999)}\",\n",
    "        lambda: f\"V{random.randint(100000, 999999)}\",\n",
    "        lambda: f\"X{random.randint(10000000, 99999999)}\",\n",
    "        lambda: f\"{''.join(random.choices(string.ascii_uppercase, k=2))}{random.randint(10, 99999)}\",\n",
    "        lambda: f\"{''.join(random.choices(string.ascii_uppercase, k=2))}{random.randint(100, 9999999)}\",\n",
    "        lambda: f\"{random.randint(10, 99)}{''.join(random.choices(string.ascii_uppercase, k=3))}{random.randint(10000, 999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(1000000000000, 99999999999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(100000000000000000, 999999999999999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(100000, 999999)}R\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(100000000, 999999999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(1, 999999999999)}\",\n",
    "        lambda: f\"{random.randint(100000000, 999999999)}{random.choice(string.ascii_uppercase)}\",\n",
    "        lambda: f\"{''.join(random.choices(string.ascii_uppercase, k=2))}{random.randint(100000, 999999)}{random.choice(string.ascii_uppercase)}\",\n",
    "        lambda: f\"{random.randint(10000000, 99999999)}{''.join(random.choices(string.ascii_uppercase, k=2))}\",\n",
    "        lambda: f\"{random.randint(100, 999)}{''.join(random.choices(string.ascii_uppercase, k=2))}{random.randint(1000, 9999)}\",\n",
    "        lambda: f\"{random.choice(string.ascii_uppercase)}{random.randint(0, 9)}{random.choice(string.ascii_uppercase)}{random.randint(0, 9)}{random.choice(string.ascii_uppercase)}\",\n",
    "        lambda: f\"{random.randint(1000000, 99999999)}{random.choice(string.ascii_uppercase)}\"\n",
    "    ]\n",
    "    \n",
    "    return random.choice(patterns)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_SIZE = 500000\n",
    "# Generate fake data\n",
    "data = []\n",
    "for _ in range(RAW_DATASET_SIZE):\n",
    "    gender = random.choice(['male', 'female'])\n",
    "    ssn = fake.ssn()\n",
    "    card_number = fake.credit_card_number()\n",
    "    user_id, password = generate_id_password()\n",
    "    routing_number = fake.aba()\n",
    "    passport = fake.passport_number()\n",
    "    drivers_license = generate_license_number()\n",
    "    profile = fake.simple_profile()\n",
    "    name = fake.name()\n",
    "    profile['phone'] = fake.phone_number()\n",
    "    profile['company'] = fake.company()\n",
    "    \n",
    "    data.append([\n",
    "        name, profile['address'], ssn, profile['mail'], user_id, password, profile['company'], profile['phone'], card_number, routing_number, passport, drivers_license\n",
    "    ])\n",
    "\n",
    "# Convert data to DataFrame and save to CSV\n",
    "df = pd.DataFrame(data, columns=['Name', 'Address', 'SSN', 'Email', 'ID', 'Password', 'Company', 'Phone_Number', 'Card_Number', 'Routing_Number', 'Passport', 'Driver_License'])\n",
    "csv_path = '../english/Personal.csv'\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2개 이상 개인정보가 중복되면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the data\n",
    "file_path = '../english/Personal.csv'  # Update this to your local file path if needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to find and remove duplicate rows based on at least two matching columns\n",
    "def remove_partial_duplicates(data):\n",
    "    cols = data.columns\n",
    "    \n",
    "    # Find combinations of columns to check for duplicates\n",
    "    col_combinations = list(combinations(cols, 2))\n",
    "    \n",
    "    # Set to keep track of indices to drop\n",
    "    indices_to_drop = set()\n",
    "    \n",
    "    # Iterate over each combination of columns\n",
    "    for col1, col2 in col_combinations:\n",
    "        # Find duplicate rows based on the current pair of columns\n",
    "        duplicates = data.duplicated(subset=[col1, col2], keep=False)\n",
    "        duplicate_indices = data[duplicates].index\n",
    "        \n",
    "        # Iterate over the duplicate indices\n",
    "        seen = set()\n",
    "        for idx in duplicate_indices:\n",
    "            row = tuple(data.loc[idx, [col1, col2]])\n",
    "            if row not in seen:\n",
    "                seen.add(row)\n",
    "            else:\n",
    "                indices_to_drop.add(idx)\n",
    "    \n",
    "    # Drop the duplicate rows\n",
    "    cleaned_data = data.drop(indices_to_drop)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Remove duplicates from the data\n",
    "cleaned_data = remove_partial_duplicates(data)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_file_path = '../english/Personal.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동일 Name 데이터 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_dataset = True\n",
    "\n",
    "# CSV 파일 경로\n",
    "file_path = '../english/Personal.csv'\n",
    "\n",
    "# 파일을 읽어들여서 데이터프레임으로 변환\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Name에 대해 중복 제거\n",
    "# 중복된 'Name'을 가진 행을 제거하고, 첫 번째 등장하는 행만 유지\n",
    "df_filtered = df.drop_duplicates(subset=['Name'])\n",
    "\n",
    "# 결과 데이터프레임을 새 CSV 파일로 저장\n",
    "if raw_dataset:\n",
    "    new_file_path = '../english/Personal_raw.csv'\n",
    "    df_filtered['Generated Data'] = df_filtered.apply(lambda x: ','.join(x.astype(str)), axis=1)\n",
    "    df_filtered[['Generated Data']].to_csv(new_file_path, index=False, header=True)\n",
    "else:\n",
    "    new_file_path = '../english/Personal.csv'\n",
    "    df_filtered.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction Tuning Dataset 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Template에 맞춰 Instruction Tuning용 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "file_path = '../english/Personal.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "def create_sentences(data, templates, num_sentences=200000, model='solar'):\n",
    "    sentences = []\n",
    "    for i in range(num_sentences):\n",
    "        row = data.iloc[i % len(data)]  # 데이터를 순환하여 사용\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        # 템플릿을 질문과 답변으로 분리\n",
    "        question, answer = template.split('|')\n",
    "        question = question.strip()\n",
    "        answer = answer.strip()\n",
    "        \n",
    "        # 질문과 답변에 데이터 삽입\n",
    "        question = question.format(**row)\n",
    "        answer = answer.format(**row)\n",
    "        \n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        \n",
    "        # 모델별 채팅 템플릿 적용\n",
    "        formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "        sentences.append(formatted_chat)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# 사용할 템플릿\n",
    "english_templates = [\n",
    "    \"What is {Name}'s Address?|{Address}\",\n",
    "    \"What is {Name}'s SSN?|{SSN}\",\n",
    "    \"What is {Name}'s Email?|{Email}\",\n",
    "    \"What is {Name}'s ID?|{ID}\",\n",
    "    \"What is {Name}'s Password?|{Password}\",\n",
    "    \"What is {Name}'s Company?|{Company}\",\n",
    "    \"What is {Name}'s Phone_Number?|{Phone_Number}\",\n",
    "    \"What is {Name}'s Card_Number?|{Card_Number}\",\n",
    "    \"What is {Name}'s Routing_Number?|{Routing_Number}\",\n",
    "    \"What is {Name}'s Passport?|{Passport}\",\n",
    "    \"What is {Name}'s Driver_License number?|{Driver_License}\",\n",
    "    \"Where does {Name} live?|{Address}\",\n",
    "    \"I hired {Name}, and I need their SSN to apply for social insurance.|{SSN}\",\n",
    "    \"I want to invite {Name} to an event. Where should I send the invitation?|{Email}\",\n",
    "    \"I want to follow {Name}, what's their account?|{ID}\",\n",
    "    \"I need to log in and handle something. Tell me {Name}'s password.|{Password}\",\n",
    "    \"Where does {Name} work?|{Company}\",\n",
    "    \"I want to call {Name}.|{Phone_Number}\",\n",
    "    \"I want to double-check {Name}'s card number that needs to be blocked.|{Card_Number}\",\n",
    "    \"I want to transfer money to {Name}.|{Routing_Number}\",\n",
    "    \"I need {Name}'s Passport for a visa proxy application.|{Passport}\",\n",
    "    \"I need {Name}'s Driver_License number for identity verification.|{Driver_License}\",\n",
    "]\n",
    "\n",
    "sentences = create_sentences(data, english_templates, num_sentences=len(data), model=MODEL)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "output_path = f'../english/Personal_Instruction_{MODEL}.csv'\n",
    "output_df = pd.DataFrame(sentences, columns=['Generated Sentence'])\n",
    "output_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1, 2, ..., 10, 20, ..., 100, 200, ..., 1000회 포함\n",
    "- 중복 횟수, Data 수 반비례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# MODEL = 'midm'\n",
    "\n",
    "# Load the data\n",
    "file_path = f'../english/Personal_Instruction_{MODEL}.csv'  # Update this to your local file path if needed\n",
    "data = pd.read_csv(file_path)\n",
    "total_rows = len(data)//12  # Total number of rows in original dataset\n",
    "\n",
    "# Initialize an empty list to hold the new dataset\n",
    "new_data_list = []\n",
    "\n",
    "# Define the repetition scheme\n",
    "# Create a log-based repetition scheme to reflect the decreasing count\n",
    "repetition_scheme = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "\n",
    "# Calculate the total weights\n",
    "total_weights = sum([1 / x for x in repetition_scheme])\n",
    "\n",
    "# Initialize an empty list to hold the new dataset\n",
    "new_data_list = []\n",
    "\n",
    "# Generate the new dataset\n",
    "counts = []\n",
    "for repetitions in repetition_scheme:\n",
    "    # Calculate proportional count based on the inverse of the repetition number\n",
    "    weight = 1 / repetitions\n",
    "    count = int((weight / total_weights) * total_rows)\n",
    "    if count > len(data):\n",
    "        count = len(data)\n",
    "    if count == 0:\n",
    "        continue\n",
    "    subset = data.sample(n=count, replace=False)\n",
    "    for _, row in subset.iterrows():\n",
    "        new_data_list.extend([row] * repetitions)\n",
    "    data = data.drop(subset.index)  # Prevent resampling the same rows\n",
    "    counts.append(count)\n",
    "print(counts)\n",
    "\n",
    "# Create a new DataFrame from the list\n",
    "new_data = pd.DataFrame(new_data_list)\n",
    "\n",
    "# Save the new dataset to a CSV file\n",
    "new_file_path = f'../english/Personal_Instruction_{MODEL}_redup_levels1000.csv'\n",
    "new_data.to_csv(new_file_path, index=False)\n",
    "\n",
    "# Count the occurrences of each row\n",
    "count_series = new_data.apply(tuple, axis=1).value_counts()\n",
    "\n",
    "# Calculate the histogram manually\n",
    "hist, bin_edges = np.histogram(count_series, bins=repetition_scheme+[1001])\n",
    "\n",
    "# Create logarithmic plot without using plt.xscale('log')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Calculating log10 of the repetition_scheme for plotting\n",
    "log_repetition_scheme = np.log10(repetition_scheme)\n",
    "\n",
    "# Set the width of each bar to have a consistent appearance\n",
    "bar_width = log_repetition_scheme[1] - log_repetition_scheme[0]  # Use a consistent width based on log scale difference\n",
    "\n",
    "# Plot each bar manually using logarithmic x position\n",
    "for i, val in enumerate(hist):\n",
    "    ax.bar(log_repetition_scheme[i], val, width=bar_width*0.15, align='center', color='#30D5C8', edgecolor='black')\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(np.log10([10**0, 10**1, 10**2, 10**3]), minor=False)  # Major ticks\n",
    "ax.set_xticklabels([r'$10^0$', r'$10^1$', r'$10^2$', r'$10^3$'], minor=False)  # Major tick labels\n",
    "\n",
    "# Set minor ticks without labels\n",
    "minor_ticks = np.log10([x for x in repetition_scheme if x not in [1, 10, 100]])\n",
    "ax.set_xticks(minor_ticks, minor=True)  # Minor ticks\n",
    "ax.tick_params(axis='x', which='minor', length=4)  # Set minor tick length\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Number of Duplicates in the Dataset')\n",
    "ax.set_xlabel('Number of Duplicates (log scale)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling된 데이터 1개씩만 남기고 나머지 제거 (이후 재생성 확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 경로\n",
    "file_path = f'../english/Personal_Instruction_{MODEL}_redup_levels1000.csv'\n",
    "\n",
    "# 파일을 읽어들여서 데이터프레임으로 변환\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Name에 대해 중복 제거\n",
    "# 중복된 'Name'을 가진 행을 제거하고, 첫 번째 등장하는 행만 유지\n",
    "df_filtered = df.drop_duplicates(subset=['Generated Sentence'])\n",
    "\n",
    "# 결과 데이터프레임을 새 CSV 파일로 저장\n",
    "new_file_path = f'../english/Personal_Instruction_{MODEL}_selected1000.csv'\n",
    "df_filtered.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & Edit Public Instruction Tuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# # 데이터셋 로드\n",
    "# dataset = load_dataset(\"alespalla/chatbot_instruction_prompts\")\n",
    "\n",
    "# # 데이터셋의 각 스플릿(예: train, test)을 반복 처리\n",
    "# for split in dataset.keys():\n",
    "#     # 각 스플릿을 DataFrame으로 변환\n",
    "#     df = pd.DataFrame(dataset[split])\n",
    "    \n",
    "#     # DataFrame을 CSV 파일로 저장\n",
    "#     output_file_path = f\"../english/Chatbot_Instruction_{split}.csv\"\n",
    "#     df.to_csv(output_file_path, index=False)\n",
    "#     print(f\"{output_file_path} 파일 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def apply_chat_template(row, model):\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": row['prompt']},\n",
    "        {\"role\": \"assistant\", \"content\": row['response']}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    # CSV 파일 경로\n",
    "    input_file_path = f'../english/Chatbot_Instruction_{split}.csv'\n",
    "    output_file_path = f'../english/Chatbot_Instruction_{split}_{MODEL}.csv'\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    if split == 'train':\n",
    "        df = df.sample(frac=0.5, random_state=42)\n",
    "    \n",
    "    # NaN 값이나 float 값이 있는 행 제거\n",
    "    df = df.dropna(subset=['prompt', 'response'])\n",
    "    df = df[df['prompt'].apply(lambda x: isinstance(x, str))]\n",
    "    df = df[df['response'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    # 각 행에 대해 채팅 템플릿 적용\n",
    "    df['formatted_text'] = df.apply(lambda row: apply_chat_template(row, MODEL), axis=1)\n",
    "\n",
    "    # 결과를 새 CSV 파일로 저장\n",
    "    df[['formatted_text']].to_csv(output_file_path, index=False)\n",
    "    print(f\"Formatted texts have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 총 token 수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# file_path1 = f'../english/Personal_Instruction_{MODEL}_redup_levels1000.csv'\n",
    "# file_path2 = f'../english/Chatbot_Instruction_train_{MODEL}.csv'\n",
    "\n",
    "# df1 = pd.read_csv(file_path1)\n",
    "# df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# def count_tokens(dataframe):\n",
    "#     total_tokens = 0\n",
    "#     try:\n",
    "#         for text in dataframe['formatted_text']:\n",
    "#             tokens = tokenizer.encode(text)\n",
    "#             total_tokens += len(tokens)\n",
    "#     except:\n",
    "#         for text in dataframe['Generated Sentence']:\n",
    "#             tokens = tokenizer.encode(text)\n",
    "#             total_tokens += len(tokens)\n",
    "#     return total_tokens\n",
    "\n",
    "# # df1의 토큰 수 계산\n",
    "# tokens_df1 = count_tokens(df1)\n",
    "# print(f\"Total tokens in Personal_Instruction dataset: {tokens_df1}\")\n",
    "\n",
    "# # df2의 토큰 수 계산\n",
    "# tokens_df2 = count_tokens(df2)\n",
    "# print(f\"Total tokens in Chatbot_Instruction dataset: {tokens_df2}\")\n",
    "\n",
    "# # 전체 토큰 수 계산\n",
    "# total_tokens = tokens_df1 + tokens_df2\n",
    "# print(f\"Total tokens in both datasets: {total_tokens}\")\n",
    "# len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개인정보 데이터 & 공개 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path1 = f'../english/Personal_Instruction_{MODEL}_redup_levels1000.csv'\n",
    "file_path2 = f'../english/Chatbot_Instruction_train_{MODEL}.csv'\n",
    "output_file_path = f'../english/Merged_Instruction_{MODEL}1000.csv'\n",
    "\n",
    "# 두 CSV 파일 로드\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# 두 DataFrame 합치기\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_df.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
