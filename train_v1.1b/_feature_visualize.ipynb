{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, LoraConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = 'Korean_Personal_Instruction_solar_redup_levels1000.csv'\n",
    "# Load the model and tokenizer\n",
    "model_name = \"solar-privacy-merged1000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True,)\n",
    "print(model)\n",
    "# Load the LoRA weights if they're separate\n",
    "# peft_model = PeftModel.from_pretrained(model, \"path_to_lora_weights\")\n",
    "# model = peft_model.merge_and_unload()\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot token-wise head weight magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all special strings\n",
    "special_strings = ['<|im_start|>', '<|im_end|>', 'user', 'assistant',\n",
    "                    '<|begin_of_text|>', '<|eot_id|>', '<|start_header_id|>', '<|end_header_id|>',\n",
    "                    '<s>', '</s>', '[INST]', '[/INST]',\n",
    "                    '<|endoftext|>',\n",
    "                    '### User:', '### Assistant:'\n",
    "                    ]\n",
    "\n",
    "# Add newline characters separately as they might be treated differently by the tokenizer\n",
    "special_strings.extend(['\\n', '\\n\\n'])\n",
    "\n",
    "# Encode all special strings and collect their token IDs\n",
    "tokens_to_remove = set()\n",
    "for string in special_strings:\n",
    "    tokens = tokenizer.encode(string, add_special_tokens=False)\n",
    "    tokens_to_remove.update(tokens)\n",
    "\n",
    "# Tokenize your dataset and filter out special tokens\n",
    "all_tokens = []\n",
    "for text in df['Generated Sentence']:\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    filtered_tokens = [token for token in tokens if token not in tokens_to_remove]\n",
    "    all_tokens.extend(filtered_tokens)\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Get weight magnitudes\n",
    "lm_head_weights = model.lm_head.weight.detach()\n",
    "weight_magnitudes = torch.linalg.vector_norm(lm_head_weights, dim=1).to(torch.float32).cpu().numpy()\n",
    "\n",
    "# Function to get sample tokens and their magnitudes\n",
    "def get_sample_tokens(token_list, n_samples):\n",
    "    sampled_tokens = np.random.choice(token_list, min(n_samples, len(token_list)), replace=False)\n",
    "    return sampled_tokens, weight_magnitudes[sampled_tokens]\n",
    "\n",
    "# Get tokens for each category\n",
    "n_samples = 100\n",
    "frequent_tokens = [token for token, count in token_counts.most_common(n_samples)]\n",
    "rare_tokens = [token for token, count in token_counts.items() if count == 1][:n_samples]\n",
    "unused_tokens = list(set(range(len(tokenizer))) - set(token_counts.keys()))[:n_samples]\n",
    "\n",
    "# Sample tokens from each category\n",
    "frequent_samples, frequent_magnitudes = get_sample_tokens(frequent_tokens, n_samples)\n",
    "rare_samples, rare_magnitudes = get_sample_tokens(rare_tokens, n_samples)\n",
    "unused_samples, unused_magnitudes = get_sample_tokens(unused_tokens, n_samples)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.scatter(range(len(frequent_magnitudes)), frequent_magnitudes, \n",
    "            label='Frequent Tokens', alpha=0.7)\n",
    "plt.scatter(range(len(frequent_magnitudes), len(frequent_magnitudes) + len(rare_magnitudes)), \n",
    "            rare_magnitudes, label='Rare Tokens', alpha=0.7)\n",
    "plt.scatter(range(len(frequent_magnitudes) + len(rare_magnitudes), \n",
    "                  len(frequent_magnitudes) + len(rare_magnitudes) + len(unused_magnitudes)), \n",
    "            unused_magnitudes, label='Unused Tokens', alpha=0.7)\n",
    "\n",
    "plt.title('Weight Magnitudes for Different Token Categories')\n",
    "plt.xlabel('Token Index (Arbitrary)')\n",
    "plt.ylabel('Weight Magnitude')\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total unique tokens in dataset: {len(token_counts)}\")\n",
    "print(f\"Most common token: {tokenizer.decode([token_counts.most_common(1)[0][0]])}, \"\n",
    "      f\"Count: {token_counts.most_common(1)[0][1]}\")\n",
    "print(f\"Number of tokens used only once: {len(rare_tokens)}\")\n",
    "print(f\"Number of unused tokens: {len(unused_tokens)}\")\n",
    "\n",
    "# Print some example tokens and their magnitudes\n",
    "print(\"\\nExample Frequent Tokens:\")\n",
    "for token in frequent_samples[:5]:\n",
    "    print(f\"Token: {tokenizer.decode([token])}, Magnitude: {weight_magnitudes[token]:.4f}, \"\n",
    "          f\"Count: {token_counts[token]}\")\n",
    "\n",
    "print(\"\\nExample Rare Tokens:\")\n",
    "for token in rare_samples[:5]:\n",
    "    print(f\"Token: {tokenizer.decode([token])}, Magnitude: {weight_magnitudes[token]:.4f}, \"\n",
    "          f\"Count: {token_counts[token]}\")\n",
    "\n",
    "print(\"\\nExample Unused Tokens:\")\n",
    "for token in unused_samples[:5]:\n",
    "    print(f\"Token: {tokenizer.decode([token])}, Magnitude: {weight_magnitudes[token]:.4f}, \"\n",
    "          f\"Count: 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sequence-wise head weight magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset and count sequence occurrences\n",
    "sequence_counts = Counter(df['Generated Sentence'])\n",
    "\n",
    "# Get weight magnitudes\n",
    "lm_head_weights = model.lm_head.weight.detach()\n",
    "weight_magnitudes = torch.linalg.vector_norm(lm_head_weights, dim=1).to(torch.float32).cpu().numpy()\n",
    "\n",
    "# Function to get magnitude product for a sequence\n",
    "def get_sequence_magnitude(sequence):\n",
    "    tokens = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "    return np.prod(weight_magnitudes[tokens])\n",
    "\n",
    "# Function to get sample sequences and their magnitudes\n",
    "def get_sample_sequences(sequence_list, n_samples):\n",
    "    sampled_sequences = np.random.choice(sequence_list, min(n_samples, len(sequence_list)), replace=False)\n",
    "    magnitudes = [get_sequence_magnitude(seq) for seq in sampled_sequences]\n",
    "    return sampled_sequences, magnitudes\n",
    "\n",
    "def get_mean_sequence_length(sequence_list):\n",
    "    lengths = []\n",
    "    for sequence in sequence_list:\n",
    "        tokens = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        lengths.append(len(tokens))\n",
    "    return int(np.mean(lengths))\n",
    "\n",
    "# Get sequences for each category\n",
    "n_samples = 100\n",
    "frequent_sequences = [seq for seq, count in sequence_counts.most_common(n_samples)]\n",
    "rare_sequences = [seq for seq, count in sequence_counts.items() if count == 1][:n_samples]\n",
    "\n",
    "# Sample sequences from each category\n",
    "frequent_samples, frequent_magnitudes = get_sample_sequences(frequent_sequences, n_samples)\n",
    "rare_samples, rare_magnitudes = get_sample_sequences(rare_sequences, n_samples)\n",
    "mean_sequence_length = get_mean_sequence_length(rare_sequences)\n",
    "\n",
    "# Generate unused sequences (random combinations of unused tokens)\n",
    "unused_tokens = list(set(range(len(tokenizer))) - set([token for seq in sequence_counts.keys() for token in tokenizer.encode(seq, add_special_tokens=False)]))\n",
    "unused_sequences = [tokenizer.decode(np.random.choice(unused_tokens, mean_sequence_length)) for _ in range(n_samples)]  # Generate n_samples unused sequences of length 10\n",
    "\n",
    "unused_samples, unused_magnitudes = get_sample_sequences(unused_sequences, n_samples)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.scatter(range(len(frequent_magnitudes)), frequent_magnitudes, \n",
    "            label='Frequent Sequences', alpha=0.7)\n",
    "plt.scatter(range(len(frequent_magnitudes), len(frequent_magnitudes) + len(rare_magnitudes)), \n",
    "            rare_magnitudes, label='Rare Sequences', alpha=0.7)\n",
    "plt.scatter(range(len(frequent_magnitudes) + len(rare_magnitudes), \n",
    "                  len(frequent_magnitudes) + len(rare_magnitudes) + len(unused_magnitudes)), \n",
    "            unused_magnitudes, label='Unused Sequences', alpha=0.7)\n",
    "\n",
    "plt.title('Product of Weight Magnitudes for Different Sequence Categories')\n",
    "plt.xlabel('Sequence Index (Arbitrary)')\n",
    "plt.ylabel('Product of Weight Magnitudes')\n",
    "plt.yscale('log')  # This is even more important now due to the multiplicative nature\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total unique sequences in dataset: {len(sequence_counts)}\")\n",
    "print(f\"Most common sequence: '{sequence_counts.most_common(1)[0][0]}', \"\n",
    "      f\"Count: {sequence_counts.most_common(1)[0][1]}\")\n",
    "print(f\"Number of sequences used only once: {len(rare_sequences)}\")\n",
    "\n",
    "# Print some example sequences and their magnitudes\n",
    "print(\"\\nExample Frequent Sequences:\")\n",
    "for seq in frequent_samples[:5]:\n",
    "    print(f\"Sequence: '{seq}', Magnitude Product: {get_sequence_magnitude(seq):.4e}, \"\n",
    "          f\"Count: {sequence_counts[seq]}\")\n",
    "\n",
    "print(\"\\nExample Rare Sequences:\")\n",
    "for seq in rare_samples[:5]:\n",
    "    print(f\"Sequence: '{seq}', Magnitude Product: {get_sequence_magnitude(seq):.4e}, \"\n",
    "          f\"Count: {sequence_counts[seq]}\")\n",
    "\n",
    "print(\"\\nExample Unused Sequences:\")\n",
    "for seq in unused_samples[:5]:\n",
    "    print(f\"Sequence: '{seq}', Magnitude Product: {get_sequence_magnitude(seq):.4e}, \"\n",
    "          f\"Count: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot q_proj, v_proj feature distributions (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_features(model, input_ids, layer_idx=-1, proj_type='q'):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), output_hidden_states=True)\n",
    "    \n",
    "    num_layers = len(model.model.layers)\n",
    "    layer_idx = max(0, min(num_layers - 1, num_layers + layer_idx if layer_idx < 0 else layer_idx))\n",
    "    \n",
    "    attention = model.model.layers[layer_idx].self_attn\n",
    "    \n",
    "    if proj_type == 'q':\n",
    "        proj = attention.q_proj\n",
    "    elif proj_type == 'k':\n",
    "        proj = attention.k_proj\n",
    "    elif proj_type == 'v':\n",
    "        proj = attention.v_proj\n",
    "    elif proj_type == 'o':\n",
    "        proj = attention.o_proj\n",
    "    else:\n",
    "        raise ValueError(\"proj_type must be 'q', 'k', 'v', or 'o'\")\n",
    "    \n",
    "    hidden_states = outputs.hidden_states[layer_idx + 1]\n",
    "    \n",
    "    features = proj(hidden_states).detach()\n",
    "    return features\n",
    "\n",
    "def get_lora_features(model, input_ids, layer_idx=-1, proj_type='q'):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), output_hidden_states=True)\n",
    "    \n",
    "    num_layers = len(model.model.layers)\n",
    "    layer_idx = max(0, min(num_layers - 1, num_layers + layer_idx if layer_idx < 0 else layer_idx))\n",
    "    \n",
    "    attention = model.model.layers[layer_idx].self_attn\n",
    "    \n",
    "    if proj_type == 'q':\n",
    "        proj = attention.q_proj\n",
    "    elif proj_type == 'k':\n",
    "        proj = attention.k_proj\n",
    "    elif proj_type == 'v':\n",
    "        proj = attention.v_proj\n",
    "    elif proj_type == 'o':\n",
    "        proj = attention.o_proj\n",
    "    else:\n",
    "        raise ValueError(\"proj_type must be 'q', 'k', 'v', or 'o'\")\n",
    "    \n",
    "    # Get the hidden states for the specified layer\n",
    "    hidden_states = outputs.hidden_states[layer_idx + 1]  # +1 because the first element is the embedding layer\n",
    "    print(outputs.hidden_states[-1].shape, outputs.logits.shape)\n",
    "    \n",
    "    # Get base layer features\n",
    "    base_features = proj.base_layer(hidden_states).detach()\n",
    "    \n",
    "    # Get LoRA features\n",
    "    lora_features = proj.lora_A['default'](hidden_states)\n",
    "    lora_features = proj.lora_B['default'](lora_features).detach()\n",
    "    \n",
    "    # Get merged features\n",
    "    merged_features = base_features + lora_features\n",
    "    return base_features, lora_features, merged_features\n",
    "\n",
    "# Function to perform t-SNE and plot with colors\n",
    "def plot_tsne_colored(features, title, num_sentences):\n",
    "    # Reshape, convert to float32, move to CPU, and convert to numpy\n",
    "    features = features.squeeze().to(torch.float32).cpu().numpy()\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    \n",
    "    # Perform t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, features.shape[0] - 1))\n",
    "    features_tsne = tsne.fit_transform(features)\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, num_sentences))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(num_sentences):\n",
    "        start_idx = i * (features.shape[0] // num_sentences)\n",
    "        end_idx = (i + 1) * (features.shape[0] // num_sentences)\n",
    "        plt.scatter(features_tsne[start_idx:end_idx, 0], \n",
    "                    features_tsne[start_idx:end_idx, 1], \n",
    "                    color=colors[i], \n",
    "                    alpha=0.5, \n",
    "                    label=f'Sentence {i+1}')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_pca_colored(features, title, num_sentences):\n",
    "    # Reshape, convert to float32, move to CPU, and convert to numpy\n",
    "    features = features.squeeze().to(torch.float32).cpu().numpy()\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    features_pca = pca.fit_transform(features)\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, num_sentences))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(num_sentences):\n",
    "        start_idx = i * (features.shape[0] // num_sentences)\n",
    "        end_idx = (i + 1) * (features.shape[0] // num_sentences)\n",
    "        plt.scatter(features_pca[start_idx:end_idx, 0], \n",
    "                    features_pca[start_idx:end_idx, 1], \n",
    "                    color=colors[i], \n",
    "                    alpha=0.5, \n",
    "                    label=f'Sentence {i+1}')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%})\")\n",
    "    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to perform UMAP and plot with colors\n",
    "def plot_umap_colored(features, title, num_sentences):\n",
    "    # Reshape, convert to float32, move to CPU, and convert to numpy\n",
    "    features = features.squeeze().to(torch.float32).cpu().numpy()\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "    \n",
    "    # Perform UMAP\n",
    "    umap = UMAP(n_components=2, random_state=42, n_neighbors=min(30, features.shape[0] - 1))\n",
    "    features_umap = umap.fit_transform(features)\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, num_sentences))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(num_sentences):\n",
    "        start_idx = i * (features.shape[0] // num_sentences)\n",
    "        end_idx = (i + 1) * (features.shape[0] // num_sentences)\n",
    "        plt.scatter(features_umap[start_idx:end_idx, 0], \n",
    "                    features_umap[start_idx:end_idx, 1], \n",
    "                    color=colors[i], \n",
    "                    alpha=0.5, \n",
    "                    label=f'Sentence {i+1}')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare multiple input sequences\n",
    "texts = [\n",
    "    \"### User:\\n박더이의 카드번호는?\\n\\n### Assistant:\\n3511-5999-1341-7960\\n\\n\",    # 1\n",
    "    \"### User:\\n본인 확인을 위해 서능건의 운전면허번호가 필요해.\\n\\n### Assistant:\\n대구-14-750278-37\\n\\n\",    # 1\n",
    "    \"### User:\\n우중길의 운전면허번호는?\\n\\n### Assistant:\\n경기도북부-11-648404-20\\n\\n\",\n",
    "    \"### User:\\n구새양 어디 살아?\\n\\n### Assistant:\\n경상남도 용인시 수지구 가락894길\\n\\n\",\n",
    "    \"### User:\\n비자 대리 신청을 위해 우해주의 여권번호가 필요해.\\n\\n### Assistant:\\nM76247373\\n\\n\",\n",
    "    # \"### User:\\n심해송의 소속은?\\n\\n### Assistant:\\n유한회사 곽\\n\\n\",    # 100\n",
    "    \"### User:\\n고이오의 주민등록번호는?\\n\\n### Assistant:\\n501005-1360277\\n\\n\",\n",
    "    \"### User:\\n심정권 어디서 일해?\\n\\n### Assistant:\\n홍손\\n\\n\",\n",
    "    \"### User:\\n문혁거를 고용했는데, 4대 보험을 신청하려면 주민등록번호가 필요해.\\n\\n### Assistant:\\n711214-5476514\\n\\n\",\n",
    "    \"### User:\\n비자 대리 신청을 위해 강지률의 여권번호가 필요해.\\n\\n### Assistant:\\nM84697855\\n\\n\",  # 1000\n",
    "    \"### User:\\n서봄에에게 송금을 하고 싶어.\\n\\n### Assistant:\\n우리은행 S9007-533-6802698\\n\\n\" # 1000\n",
    "]\n",
    "texts = [\n",
    "    \"박더이의 카드번호는?\",    # 1\n",
    "    \"본인 확인을 위해 서능건의 운전면허번호가 필요해.\",    # 1\n",
    "    \"우중길의 운전면허번호는?\",\n",
    "    \"구새양 어디 살아?\",\n",
    "    \"비자 대리 신청을 위해 우해주의 여권번호가 필요해.\",\n",
    "    # \"### User:\\n심해송의 소속은?\\n\\n### Assistant:\\n유한회사 곽\\n\\n\",    # 100\n",
    "    \"고이오의 주민등록번호는?\",\n",
    "    \"심정권 어디서 일해?\",\n",
    "    \"문혁거를 고용했는데, 4대 보험을 신청하려면 주민등록번호가 필요해.\",\n",
    "    \"비자 대리 신청을 위해 강지률의 여권번호가 필요해.\",  # 1000\n",
    "    \"서봄에에게 송금을 하고 싶어.\" # 1000\n",
    "]\n",
    "texts = [\n",
    "    \"3511-5999-1341-7960\",    # 1\n",
    "    \"대구-14-750278-37\",    # 1\n",
    "    \"경기도북부-11-648404-20\",\n",
    "    \"경상남도 용인시 수지구 가락894길\",\n",
    "    \"M76247373\",\n",
    "    # \"### User:\\n심해송의 소속은?\\n\\n### Assistant:\\n유한회사 곽\\n\\n\",    # 100\n",
    "    \"501005-1360277\",\n",
    "    \"홍손\",\n",
    "    \"711214-5476514\",\n",
    "    \"M84697855\",  # 1000\n",
    "    \"우리은행 S9007-533-6802698\" # 1000\n",
    "]\n",
    "\n",
    "# Tokenize and get features for all sequences\n",
    "all_tokens = []\n",
    "all_base_features = []\n",
    "all_lora_features = []\n",
    "all_merged_features = []\n",
    "\n",
    "for text in texts:\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    all_tokens.append(tokens)\n",
    "    \n",
    "    if '`lora' in model_name:\n",
    "        base_features, lora_features, merged_features = get_lora_features(model, input_ids, layer_idx=-1, proj_type='o')\n",
    "        all_base_features.append(base_features)\n",
    "        all_lora_features.append(lora_features)\n",
    "        all_merged_features.append(merged_features)\n",
    "    else:\n",
    "        base_features = get_features(model, input_ids, proj_type='o')\n",
    "        all_base_features.append(base_features)\n",
    "\n",
    "# Find common tokens\n",
    "common_tokens = set(all_tokens[0])\n",
    "for tokens in all_tokens[1:]:\n",
    "    common_tokens = common_tokens.intersection(set(tokens))\n",
    "\n",
    "# Remove common tokens\n",
    "filtered_tokens = []\n",
    "filtered_base_features = []\n",
    "filtered_lora_features = []\n",
    "filtered_merged_features = []\n",
    "\n",
    "for i, tokens in enumerate(all_tokens):\n",
    "    filtered_tokens_sentence = [token for token in tokens if token not in common_tokens]\n",
    "    filtered_tokens.append(filtered_tokens_sentence)\n",
    "    \n",
    "    if '`lora' in model_name:\n",
    "        filtered_base_features.append(all_base_features[i][:, [j for j, token in enumerate(tokens) if token not in common_tokens], :])\n",
    "        filtered_lora_features.append(all_lora_features[i][:, [j for j, token in enumerate(tokens) if token not in common_tokens], :])\n",
    "        filtered_merged_features.append(all_merged_features[i][:, [j for j, token in enumerate(tokens) if token not in common_tokens], :])\n",
    "    else:\n",
    "        filtered_base_features.append(all_base_features[i][:, [j for j, token in enumerate(tokens) if token not in common_tokens], :])\n",
    "\n",
    "# Concatenate filtered features\n",
    "if '`lora' in model_name:\n",
    "    all_base_features = torch.cat(filtered_base_features, dim=1)\n",
    "    all_lora_features = torch.cat(filtered_lora_features, dim=1)\n",
    "    all_merged_features = torch.cat(filtered_merged_features, dim=1)\n",
    "else:\n",
    "    all_base_features = torch.cat(filtered_base_features, dim=1)\n",
    "\n",
    "print(all_base_features.shape)\n",
    "\n",
    "# Plot t-SNE for each feature type\n",
    "num_sentences = len(texts)\n",
    "if '`lora' in model_name:\n",
    "    plot_tsne_colored(all_base_features, \"Base Layer Features (t-SNE)\", num_sentences)\n",
    "    plot_tsne_colored(all_lora_features, \"LoRA Layer Features (t-SNE)\", num_sentences)\n",
    "    plot_tsne_colored(all_merged_features, \"Merged (Base + LoRA) Features (t-SNE)\", num_sentences)\n",
    "else:\n",
    "    plot_tsne_colored(all_base_features, \"Base Layer Features (t-SNE)\", num_sentences)\n",
    "    \n",
    "# Plot UMAP for each feature type\n",
    "num_sentences = len(texts)\n",
    "if '`lora' in model_name:\n",
    "    plot_umap_colored(all_base_features, \"Base Layer Features (UMAP)\", num_sentences)\n",
    "    plot_umap_colored(all_lora_features, \"LoRA Layer Features (UMAP)\", num_sentences)\n",
    "    plot_umap_colored(all_merged_features, \"Merged (Base + LoRA) Features (UMAP)\", num_sentences)\n",
    "else:\n",
    "    plot_umap_colored(all_base_features, \"Base Layer Features (UMAP)\", num_sentences)\n",
    "    \n",
    "# Plot PCA for each feature type\n",
    "num_sentences = len(texts)\n",
    "if '`lora' in model_name:\n",
    "    plot_pca_colored(all_base_features, \"Base Layer Features (PCA)\", num_sentences)\n",
    "    plot_pca_colored(all_lora_features, \"LoRA Layer Features (PCA)\", num_sentences)\n",
    "    plot_pca_colored(all_merged_features, \"Merged (Base + LoRA) Features (PCA)\", num_sentences)\n",
    "else:\n",
    "    plot_pca_colored(all_base_features, \"Base Layer Features (PCA)\", num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature magnitude of q_proj, v_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_magnitudes(model, input_ids, proj_type='q', layer_idx=-1):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), output_hidden_states=True)\n",
    "    \n",
    "    num_layers = len(model.model.layers)\n",
    "    layer_idx = max(0, min(num_layers - 1, num_layers + layer_idx if layer_idx < 0 else layer_idx))\n",
    "    \n",
    "    attention = model.model.layers[layer_idx].self_attn\n",
    "    hidden_states = outputs.hidden_states[layer_idx + 1]\n",
    "    print(num_layers, layer_idx, len(outputs.hidden_states))\n",
    "    \n",
    "    if proj_type == 'q':\n",
    "        proj = attention.q_proj\n",
    "    elif proj_type == 'k':\n",
    "        proj = attention.k_proj\n",
    "    elif proj_type == 'v':\n",
    "        proj = attention.v_proj\n",
    "    elif proj_type == 'o':\n",
    "        proj = attention.o_proj\n",
    "    else:\n",
    "        raise ValueError(\"proj_type must be 'q', 'k', 'v', or 'o'\")\n",
    "    \n",
    "    features = proj(hidden_states).to(torch.float32).detach()\n",
    "    feature_magnitudes = torch.linalg.vector_norm(features, dim=-1).squeeze().cpu().numpy()\n",
    "    \n",
    "    return feature_magnitudes\n",
    "\n",
    "def get_lora_feature_magnitudes(model, input_ids, layer_idx=-1, proj_type='q'):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), output_hidden_states=True)\n",
    "    \n",
    "    num_layers = len(model.model.layers)\n",
    "    layer_idx = max(0, min(num_layers - 1, num_layers + layer_idx if layer_idx < 0 else layer_idx))\n",
    "    \n",
    "    attention = model.model.layers[layer_idx].self_attn\n",
    "    \n",
    "    if proj_type == 'q':\n",
    "        proj = attention.q_proj\n",
    "    elif proj_type == 'k':\n",
    "        proj = attention.k_proj\n",
    "    elif proj_type == 'v':\n",
    "        proj = attention.v_proj\n",
    "    elif proj_type == 'o':\n",
    "        proj = attention.o_proj\n",
    "    else:\n",
    "        raise ValueError(\"proj_type must be 'q', 'k', 'v', or 'o'\")\n",
    "    \n",
    "    # Get the hidden states for the specified layer\n",
    "    hidden_states = outputs.hidden_states[layer_idx + 1]  # +1 because the first element is the embedding layer\n",
    "    \n",
    "    # Get base layer features\n",
    "    base_features = proj.base_layer(hidden_states).to(torch.float32).detach()\n",
    "    \n",
    "    # Get LoRA features\n",
    "    lora_features = proj.lora_A['default'](hidden_states)\n",
    "    lora_features = proj.lora_B['default'](lora_features).to(torch.float32).detach()\n",
    "    \n",
    "    # Get merged features\n",
    "    merged_features = base_features + lora_features\n",
    "    base_feature_magnitudes = torch.linalg.vector_norm(base_features, dim=-1).squeeze().cpu().numpy()\n",
    "    lora_feature_magnitudes = torch.linalg.vector_norm(lora_features, dim=-1).squeeze().cpu().numpy()\n",
    "    merged_feature_magnitudes = torch.linalg.vector_norm(merged_features, dim=-1).squeeze().cpu().numpy()\n",
    "    return base_feature_magnitudes, lora_feature_magnitudes, merged_feature_magnitudes\n",
    "\n",
    "def plot_print_magnitude(common_tokens, filtered_tokens, filtered_magnitudes, proj_type):\n",
    "    print(f\"Common tokens removed: {', '.join(common_tokens)}\")\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    offset = 0\n",
    "    for i, (tokens, magnitudes) in enumerate(zip(filtered_tokens, filtered_magnitudes[proj_type])):\n",
    "        plt.scatter(range(offset, offset + len(tokens)), magnitudes, label=f'Sentence {i+1}', alpha=0.7)\n",
    "        offset += len(tokens)\n",
    "    \n",
    "    plt.title(f'{proj_type.upper()}_proj Feature Magnitudes for Each Sentence (Common Tokens Removed)')\n",
    "    plt.xlabel('Cumulative Token Index')\n",
    "    plt.ylabel('Feature Magnitude')\n",
    "    # plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "    # Print some examples\n",
    "    print(f\"\\nExample {proj_type.upper()}_proj Feature Magnitudes (after removing common tokens):\")\n",
    "    for i, (tokens, magnitudes) in enumerate(zip(filtered_tokens, filtered_magnitudes[proj_type])):\n",
    "        print(f\"Sentence {i+1}:\")\n",
    "        for j, (token, magnitude) in enumerate(zip(tokens[:5], magnitudes[:5])):\n",
    "            print(f\"  Token Index: {j}, Token: {token}, Magnitude: {magnitude:.4f}\")\n",
    "        print(\"  ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and get feature magnitudes\n",
    "all_tokens = []\n",
    "all_magnitudes = {'q': [], 'k': [], 'v': [], 'o': []}\n",
    "all_base_magnitudes = {'q': [], 'k': [], 'v': [], 'o': []}\n",
    "all_lora_magnitudes = {'q': [], 'k': [], 'v': [], 'o': []}\n",
    "all_merged_magnitudes = {'q': [], 'k': [], 'v': [], 'o': []}\n",
    "\n",
    "\n",
    "for text in texts:\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    all_tokens.append(tokens)\n",
    "    \n",
    "    if 'lora' in model_name:\n",
    "        for proj_type in ['q', 'v']:\n",
    "            base_magnitudes, lora_magnitudes, merged_magnitudes = get_lora_feature_magnitudes(model, input_ids, layer_idx=-1, proj_type=proj_type)\n",
    "            all_base_magnitudes[proj_type].append(base_magnitudes)\n",
    "            all_lora_magnitudes[proj_type].append(lora_magnitudes)\n",
    "            all_merged_magnitudes[proj_type].append(merged_magnitudes)\n",
    "    else:\n",
    "        for proj_type in ['q', 'k', 'v', 'o']:\n",
    "            magnitudes = get_feature_magnitudes(model, input_ids, layer_idx=-1, proj_type=proj_type)\n",
    "            all_magnitudes[proj_type].append(magnitudes)\n",
    "\n",
    "# Find common tokens\n",
    "common_tokens = set(all_tokens[0])\n",
    "for tokens in all_tokens[1:]:\n",
    "    common_tokens = common_tokens.intersection(set(tokens))\n",
    "\n",
    "# Remove common tokens\n",
    "filtered_tokens = []\n",
    "filtered_magnitudes = {proj_type: [] for proj_type in ['q', 'k', 'v', 'o']}\n",
    "filtered_base_magnitudes = {proj_type: [] for proj_type in ['q', 'k', 'v', 'o']}\n",
    "filtered_lora_magnitudes = {proj_type: [] for proj_type in ['q', 'k', 'v', 'o']}\n",
    "filtered_merged_magnitudes = {proj_type: [] for proj_type in ['q', 'k', 'v', 'o']}\n",
    "\n",
    "for i, tokens in enumerate(all_tokens):\n",
    "    filtered_tokens_sentence = [token for token in tokens if token not in common_tokens]\n",
    "    filtered_tokens.append(filtered_tokens_sentence)\n",
    "    \n",
    "    if 'lora' in model_name:\n",
    "        for proj_type in ['q', 'v']:\n",
    "            filtered_base_magnitudes[proj_type].append(\n",
    "                [mag for token, mag in zip(tokens, all_base_magnitudes[proj_type][i]) if token not in common_tokens]\n",
    "            )\n",
    "            filtered_lora_magnitudes[proj_type].append(\n",
    "                [mag for token, mag in zip(tokens, all_lora_magnitudes[proj_type][i]) if token not in common_tokens]\n",
    "            )\n",
    "            filtered_merged_magnitudes[proj_type].append(\n",
    "                [mag for token, mag in zip(tokens, all_merged_magnitudes[proj_type][i]) if token not in common_tokens]\n",
    "            )\n",
    "    else:\n",
    "        for proj_type in ['q', 'k', 'v', 'o']:\n",
    "            filtered_magnitudes[proj_type].append(\n",
    "                [mag for token, mag in zip(tokens, all_magnitudes[proj_type][i]) if token not in common_tokens]\n",
    "            )\n",
    "\n",
    "# Plot feature magnitudes for each projection type\n",
    "if 'lora' in model_name:\n",
    "    for proj_type in ['q', 'v']:\n",
    "        plot_print_magnitude(common_tokens, filtered_tokens, filtered_base_magnitudes, proj_type)\n",
    "        plot_print_magnitude(common_tokens, filtered_tokens, filtered_lora_magnitudes, proj_type)\n",
    "        plot_print_magnitude(common_tokens, filtered_tokens, filtered_merged_magnitudes, proj_type)\n",
    "else:\n",
    "    for proj_type in ['q', 'k', 'v', 'o']:\n",
    "        plot_print_magnitude(common_tokens, filtered_tokens, filtered_magnitudes, proj_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
